{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980170207_1232263959","id":"20151224-130250_1885393244","dateCreated":"Dec 24, 2015 1:02:50 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:192","text":"/**\n * This anomaly detection code mainly follows the tutorial from Sean Owen, Cloudera.\n * Couple of modifcations have been made to fit personal interest:\n * \t\t- Instead of training multiple clusters, the code only trains on \"normal\" data points\n * \t\t- Only one cluster center is recorded and threshold is set to the last of \n *\t\t  the furthest 3000 data points\n *\t\t- During later validating stage, all points that are further than the threshold\n *\t\t  is labeled as \"anomaly\"\n *\n * Video: https://www.youtube.com/watch?v=TC5cKYBZAeI\n * Slides-1: http://www.slideshare.net/CIGTR/anomaly-detection-with-apache-spark\n * Slides-2: http://www.slideshare.net/CIGTR/anomaly-detection-with-apache-spark-2\n */ ","dateUpdated":"Dec 24, 2015 1:35:51 PM"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980024160_-742834391","id":"20151224-130024_2027641500","dateCreated":"Dec 24, 2015 1:00:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:90","text":"//************************************\n//         Training\n//************************************\n\n// load the data\nval rawData = sc.textFile(\"dataset/ad.train.csv\", 120)\nrawData.count","dateUpdated":"Dec 24, 2015 1:48:42 PM","dateFinished":"Dec 24, 2015 1:02:03 PM","dateStarted":"Dec 24, 2015 1:02:01 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"rawData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at textFile at <console>:24\nres2: Long = 766639\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450979935165_1035735536","id":"20151224-125855_148192554","dateCreated":"Dec 24, 2015 12:58:55 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:21","text":"import scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}","dateUpdated":"Dec 24, 2015 1:02:05 PM","dateFinished":"Dec 24, 2015 1:02:06 PM","dateStarted":"Dec 24, 2015 1:02:05 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"import scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450979974447_-1979310086","id":"20151224-125934_1313969834","dateCreated":"Dec 24, 2015 12:59:34 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69","dateUpdated":"Dec 24, 2015 1:09:07 PM","dateFinished":"Dec 24, 2015 1:02:11 PM","dateStarted":"Dec 24, 2015 1:02:09 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"dataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] = MapPartitionsRDD[4] at map at <console>:29\n"},"text":"// parse the data\nval dataAndLabel = rawData.map { line =>\n\tval buffer = ArrayBuffer[String]()\n\tbuffer.appendAll(line.split(\",\"))\n\tbuffer.remove(1, 3) // remove categorial attributes\n\tval label = buffer.remove(buffer.length-1)\n\tval vector = Vectors.dense(buffer.map(_.toDouble).toArray) \n\t(vector, label)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980129119_728030006","id":"20151224-130209_551414281","dateCreated":"Dec 24, 2015 1:02:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:171","dateUpdated":"Dec 24, 2015 1:07:45 PM","dateFinished":"Dec 24, 2015 1:07:46 PM","dateStarted":"Dec 24, 2015 1:07:45 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"res7: Array[(org.apache.spark.mllib.linalg.Vector, String)] = Array(([0.0,215.0,45076.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],normal.))\nres8: Array[org.apache.spark.mllib.linalg.Vector] = Array([0.0,215.0,45076.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\nres9: Array[String] = Array(normal.)\n"},"text":"// checkout how data look like\ndataAndLabel.take(1)\ndataAndLabel.map(_._1).take(1)\ndataAndLabel.map(_._2).take(1)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980465528_-823050986","id":"20151224-130745_211188629","dateCreated":"Dec 24, 2015 1:07:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:238","dateUpdated":"Dec 24, 2015 1:07:55 PM","dateFinished":"Dec 24, 2015 1:07:56 PM","dateStarted":"Dec 24, 2015 1:07:55 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[7] at map at <console>:32\n"},"text":"// training data\nval data = dataAndLabel.map(_._1).cache()"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980475832_-1936667544","id":"20151224-130755_1820166210","dateCreated":"Dec 24, 2015 1:07:55 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257","dateUpdated":"Dec 24, 2015 1:30:03 PM","dateFinished":"Dec 24, 2015 1:31:27 PM","dateStarted":"Dec 24, 2015 1:30:03 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"dataArray: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[111] at map at <console>:67\nnumCols: Int = 38\nn: Long = 766639\nsums: Array[Double] = Array(1.2411869E8, 1.313453168E9, 2.780595069E9, 6.0, 0.0, 32.0, 43833.0, 78.0, 623371.0, 34568.0, 279.0, 162.0, 57729.0, 5268.0, 310.0, 4414.0, 0.0, 2.0, 3405.0, 6913326.0, 9303874.0, 1282.3899999999994, 1497.14, 39723.72000000001, 39953.109999999986, 756260.9699999996, 13292.690000000002, 113111.09999999995, 1.05413645E8, 1.57255352E8, 658529.119999999, 31197.48000000026, 88231.3499999989, 19878.400000000052, 1776.000000000001, 916.8499999999987, 41431.800000000105, 40048.72000000005)\nsumSquares: Array[Double] = Array(1.3452455802400472E23, 9.284520970311343E31, 5.395641627069102E28, 36.0, 0.0, 39124.0, 1.1772419567E10, 1072.0, 3.536030557E9, 3.156349418109138E15, 2653.0, 2766.0, 3.145002086882085E15, 1.9135478E8, 3082.0, 811666.0, 0.0, 2.0, 222039.0, 6.589397919088578E15, 8.890420300115038E15, 8672.91353053, 6644.889076719998, 7.194289320501961E7, 6.959693378997868E7, 4.720408616442115E9, 3196219.257420431, 7.741286569367279E7, 4.6465907883593298E18, 1.3114568325665786E19, 3.5146533067086024E9, 2.4546624048921317E7, 9.416734826826265E7, 127424.75263790124, 44047.362955920784, 6052.879212650257, 6.808441269713098E7, 6.343286915490572E7)\nstdevs: Array[Double] = Array(4.188951504920095E8, 1.1004854284147182E13, 2.6529318514282822E11, 0.006852602065360522, 0.0, 0.22590516646696115, 123.91884742877625, 0.037393862848970776, 67.90963885528959, 64164.83520144945, 0.05882539605827488, 0.060065894102794073, 64049.39246761144, 15.798802216301345, 0.06340332456453049, 1.0289314064177897, 0.0, 0.0016151727429825082, 0.5381512910778905, 92710.17712983645, 107687.59785070404, 0.10634898254620986, 0.09307924440243052, 9.687066531565373, 9.527810314981625, 78.46211700769851, 2.04177164574272, 10.047645421832357, 2461907.654089318, 4136009.6672808025, 67.70345626656726, 5.658342087376738, 11.082335640365384, 0.406865910885403, 0.23968685578859572, 0.08884770397884835, 9.423695657481783, 9.096086077247268)\nmeans: Array[Double] = Array(161.89978594879727, 1713.2616107450833, 3626.994020653789, 7.826369386373508E-6, 0.0, 4.1740636727325376E-5, 0.05717554155215167, 1.017428020228556E-4, 0.8131219517921734, 0.04509032282469324, 3.6392617646636814E-4, 2.1131197343208472E-4, 0.07530141305099271, 0.00687155232123594, 4.043624182959646E-4, 0.005757599078575444, 0.0, 2.608789795457836E-6, 0.004441464626766966, 9.01770716073667, 12.13592577471274, 0.0016727429728985864, 0.0019528617771858725, 0.051815417686812186, 0.0521146328324022, 0.9864629506195218, 0.017338917013092214, 0.14754154171650535, 137.50102068900748, 205.123078789365, 0.858982024133913, 0.04069383373400031, 0.11508852275973294, 0.025929283535014593, 0.00231660533836656, 0.0011959344619827568, 0.05404342852372512, 0.05223934602857414)\nnormalize: (v: org.apache.spark.mllib.linalg.Vector)org.apache.spark.mllib.linalg.Vector\nnormalizedData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[112] at map at <console>:84\nnormalizedDataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] = ZippedPartitionsRDD2[114] at zip at <console>:86\n"},"text":"// normalize training data\nval dataArray = data.map(_.toArray)\nval numCols = dataArray.first().length\nval n = dataArray.count()\n\nval sums = dataArray.reduce((a, b) => a.zip(b).map(t => t._1 + t._2))\nval sumSquares = dataArray.fold(new Array[Double](numCols)) (\n\t(a,b) => a.zip(b).map(t => t._1 + t._2 * t._2)\n\t)\nval stdevs = sumSquares.zip(sums).map { case\n\t(sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n\n}\nval means = sums.map(_ / n)\n\n// normaization function\ndef normalize(v: Vector) = {\n\tval normed = (v.toArray, means, stdevs).zipped.map { \n\t\tcase (value, mean, 0) => (value - mean) / 1 // if stdev is 0\n\t\tcase (value, mean, stdev) => (value - mean) / stdev\n\t\t}\n\tVectors.dense(normed)\n}\n\n// apply the function\nval normalizedData = data.map(normalize(_))\n\n// put the label back\nval normalizedDataAndLabel = normalizedData.zip(dataAndLabel.values)\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980538737_-35161859","id":"20151224-130858_521933677","dateCreated":"Dec 24, 2015 1:08:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:282","dateUpdated":"Dec 24, 2015 1:09:15 PM","dateFinished":"Dec 24, 2015 1:09:55 PM","dateStarted":"Dec 24, 2015 1:09:15 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.clustering._\nkmeans: org.apache.spark.mllib.clustering.KMeans = org.apache.spark.mllib.clustering.KMeans@5411f5\nres23: kmeans.type = org.apache.spark.mllib.clustering.KMeans@5411f5\nres24: kmeans.type = org.apache.spark.mllib.clustering.KMeans@5411f5\nmodel: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@73d9bfa6\n"},"text":"// train the model\nimport org.apache.spark.mllib.clustering._\nval kmeans = new KMeans()\nkmeans.setK(1) // find that one center\nkmeans.setRuns(10)\nval model = kmeans.run(normalizedData)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980555810_668390637","id":"20151224-130915_1509764392","dateCreated":"Dec 24, 2015 1:09:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:307","dateUpdated":"Dec 24, 2015 1:09:58 PM","dateFinished":"Dec 24, 2015 1:10:02 PM","dateStarted":"Dec 24, 2015 1:09:58 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"[2.670514605111177E-20,7.250679545857106E-25,3.2938248932737836E-23,-1.872691374892059E-16,0.0,-2.144796584038696E-17,-6.069566954993101E-18,3.0572358935278095E-16,-1.0119575002571815E-16,-2.2073267219843697E-21,-3.114513883650898E-16,-3.110899253206237E-17,-1.5569742772902257E-19,3.46587449943832E-17,-3.270499089762803E-16,8.45221085643208E-17,0.0,1.0712513419744075E-16,8.334596342732729E-16,1.1359846884107648E-18,1.7540950024421153E-19,3.0062603359748996E-16,-8.146820925274191E-16,5.280140934162351E-17,4.965111988485367E-16,1.8345408042053016E-18,-2.825992227901426E-16,2.11173198375324E-16,5.880653276505114E-19,-2.110163616456939E-19,1.006025799014661E-16,-1.871081010687547E-16,2.259653783488594E-16,-1.862767360664827E-15,6.357567269813054E-16,3.7052742542731693E-16,-2.699294796674466E-16,-4.461102748534956E-16]\nclusterAndLabel: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[43] at map at <console>:59\nclusterLabelCount: scala.collection.Map[(Int, String),Long] = Map((0,normal.) -> 766639)\n0           normal.  766639\n"},"text":"// see the centroid\nmodel.clusterCenters.foreach(centroid =>\n\tprintln(centroid.toString))\n\nval clusterAndLabel = dataAndLabel.map { case\n\t(data, label) => (model.predict(data), label)\n}\nval clusterLabelCount = clusterAndLabel.countByValue \n\nclusterLabelCount.toList.sorted.foreach { case\n\t((cluster, label), count) =>\n\t\tprintln(f\"$cluster%1s$label%18s$count%8s\")\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980598437_1111697991","id":"20151224-130958_806366288","dateCreated":"Dec 24, 2015 1:09:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:335","dateUpdated":"Dec 24, 2015 1:10:05 PM","dateFinished":"Dec 24, 2015 1:10:05 PM","dateStarted":"Dec 24, 2015 1:10:05 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"distToCentroid: (datum: org.apache.spark.mllib.linalg.Vector, model: org.apache.spark.mllib.clustering.KMeansModel)Double\n"},"text":"// calculate distance between data point to centroid\ndef distToCentroid(datum: Vector, model: KMeansModel) = {\n\tval centroid = model.clusterCenters(model.predict(datum)) // if more than 1 center\n\tVectors.sqdist(datum, centroid)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980605477_-1262602795","id":"20151224-131005_1663896157","dateCreated":"Dec 24, 2015 1:10:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:356","dateUpdated":"Dec 24, 2015 1:24:45 PM","dateFinished":"Dec 24, 2015 1:24:48 PM","dateStarted":"Dec 24, 2015 1:24:45 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"distances: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[98] at map at <console>:85\nthreshold: Double = 10.507551778111415\n"},"text":"// decide threshold for anormalies\nval distances = normalizedData.map(d => distToCentroid(d, model)) // calculate the distance between datum and the centroid\nval threshold = distances.top(3000).last // pick the last of the furthest 3000 points as the threshold"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980615469_2137285317","id":"20151224-131015_816765510","dateCreated":"Dec 24, 2015 1:10:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:375","dateUpdated":"Dec 24, 2015 1:48:57 PM","dateFinished":"Dec 24, 2015 1:21:30 PM","dateStarted":"Dec 24, 2015 1:21:28 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"rawTestdata: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[86] at textFile at <console>:52\nres123: Long = 4898431\n"},"text":"//************************************\n//         Testing\n//************************************\n// load the data\nval rawTestdata = sc.textFile(\"dataset/ad.all.csv\", 120)\nrawTestdata.count"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980730129_-558533090","id":"20151224-131210_658972449","dateCreated":"Dec 24, 2015 1:12:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:401","dateUpdated":"Dec 24, 2015 1:21:33 PM","dateFinished":"Dec 24, 2015 1:21:34 PM","dateStarted":"Dec 24, 2015 1:21:33 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"testdataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] = MapPartitionsRDD[87] at map at <console>:51\n"},"text":"// parse input\nval testdataAndLabel = rawTestdata.map { line =>\n\tval buffer = ArrayBuffer[String]()\n\tbuffer.appendAll(line.split(\",\"))\n\tbuffer.remove(1, 3) // remove categorial attributes\n\tval label = buffer.remove(buffer.length-1)\n\tval vector = Vectors.dense(buffer.map(_.toDouble).toArray) \n\t(vector, label)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980741776_1198831320","id":"20151224-131221_95996253","dateCreated":"Dec 24, 2015 1:12:21 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:421","dateUpdated":"Dec 24, 2015 1:21:34 PM","dateFinished":"Dec 24, 2015 1:21:35 PM","dateStarted":"Dec 24, 2015 1:21:34 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"testdata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[88] at map at <console>:53\n"},"text":"// validation data\nval testdata = testdataAndLabel.map(_._1).cache()"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980751378_-1016552847","id":"20151224-131231_847306018","dateCreated":"Dec 24, 2015 1:12:31 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:440","dateUpdated":"Dec 24, 2015 1:31:47 PM","dateFinished":"Dec 24, 2015 1:31:48 PM","dateStarted":"Dec 24, 2015 1:31:47 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"normalizedTestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[115] at map at <console>:89\nnormalizedTestDataAndLabel: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] = ZippedPartitionsRDD2[117] at zip at <console>:90\n"},"text":"// normalize validation data\nval normalizedTestData = testdata.map(normalize(_))\nval normalizedTestDataAndLabel = normalizedTestData.zip(testdataAndLabel.values) // put label back"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980764512_-448632822","id":"20151224-131244_870665071","dateCreated":"Dec 24, 2015 1:12:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:461","dateUpdated":"Dec 24, 2015 1:31:57 PM","dateFinished":"Dec 24, 2015 1:31:57 PM","dateStarted":"Dec 24, 2015 1:31:57 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"distToCentroid: (datum: org.apache.spark.mllib.linalg.Vector, model: org.apache.spark.mllib.clustering.KMeansModel)Double\n"},"text":"// define distance measure\ndef distToCentroid(datum: Vector, model: KMeansModel) = {\n\tval centroid = model.clusterCenters(0) // the one center\n\tVectors.sqdist(datum, centroid)\n}"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980814548_578924221","id":"20151224-131334_232414287","dateCreated":"Dec 24, 2015 1:13:34 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:488","dateUpdated":"Dec 24, 2015 1:31:58 PM","dateFinished":"Dec 24, 2015 1:31:59 PM","dateStarted":"Dec 24, 2015 1:31:58 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"testDistances: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[118] at map at <console>:115\n"},"text":"// calculate distance of validation data points\nval testDistances = normalizedTestData.map(d => distToCentroid(d, model))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980895045_-1351052883","id":"20151224-131455_1289150270","dateCreated":"Dec 24, 2015 1:14:55 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:512","dateUpdated":"Dec 24, 2015 1:48:22 PM","dateFinished":"Dec 24, 2015 1:56:53 PM","dateStarted":"Dec 24, 2015 1:48:22 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"anomalies: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, String)] = MapPartitionsRDD[123] at filter at <console>:123\ntotal: Long = 874423\ntrueLabel: Long = 871043\n0.0\n"},"text":"// get the anomalies\nval anomalies = normalizedTestDataAndLabel.filter(\n\td => distToCentroid(d._1, model) > threshold  // threshold is calculated during training\n\t)\n\nval total = anomalies.count()\nval trueLabel = anomalies.filter(x => x._2 != \"normal.\").count // count true \"anomalies\""},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1450980906901_69070632","id":"20151224-131506_1394890797","dateCreated":"Dec 24, 2015 1:15:06 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:533"}],"name":"Anomaly Detection","id":"2BA5XQHDE","angularObjects":{"2B7DU42WR":[],"2B6W4YZB4":[],"2B9RPMYXJ":[],"2B94PD98S":[],"2B7BK5KJC":[],"2B6R9CXR1":[],"2B7M935JB":[],"2B8GWTH6Z":[],"2B7YV7QK1":[],"2B8BUNH3K":[],"2B6NANY4F":[],"2B7EEWDAY":[],"2B9MNJ9MP":[],"2B9T5UWVQ":[]},"config":{"looknfeel":"default"},"info":{}}